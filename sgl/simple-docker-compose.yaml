services:
  sglang-chat:
    image: lmsysorg/sglang:latest-runtime
    container_name: sglang-chat
    volumes:
      - hf_cache:/root/.cache/huggingface
      # If you use modelscope, you need mount this directory
      # - ${HOME}/.cache/modelscope:/root/.cache/modelscope
    restart: always
    # network_mode: host # required by RDMA
    privileged: false # required by RDMA
    ports:
      - 30000:30000
    entrypoint: python3 -m sglang.launch_server
    command: >
      --model-path google/gemma-3-270m-it-qat-q4_0-unquantized
      --attention-backend flashinfer
      --dtype bfloat16
      --context-length 2048
      --mem-fraction-static 0.1
      --max-total-tokens 2048
      --max-prefill-tokens 512
      --trust-remote-code
      --log-level warning
      --host 0.0.0.0
      --port 30000
    ulimits:
      memlock: -1
      stack: 67108864
    # ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  sglang-embedding:
    image: lmsysorg/sglang:latest-runtime
    container_name: sglang-embedding
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: always
    privileged: false
    ports:
      - 30001:30000
    entrypoint: python3 -m sglang.launch_server
    command: >
      --model-path Qwen/Qwen3-Embedding-0.6B
      --is-embedding
      --json-model-override-args '{"matryoshka_dimensions": [128, 256, 512, 1024]}'
      --attention-backend flashinfer
      --dtype bfloat16
      --context-length 8192
      --cpu-offload-gb 2
      --mem-fraction-static 0.1
      --kv-cache-dtype fp8_e4m3
      --trust-remote-code
      --log-level warning
      --host 0.0.0.0
      --port 30000
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  sglang-reranker:
    image: lmsysorg/sglang:latest-runtime
    container_name: sglang-reranker
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: always
    privileged: false
    ports:
      - 30002:30000
    entrypoint: python3 -m sglang.launch_server
    command: >
      --model-path BAAI/bge-reranker-v2-m3
      --disable-radix-cache
      --is-embedding
      --attention-backend flashinfer
      --context-length 512
      --cpu-offload-gb 2
      --mem-fraction-static 0.2
      --chunked-prefill-size -1
      --log-level warning
      --host 0.0.0.0
      --port 30000
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  sglang-multimodal:
    image: lmsysorg/sglang:latest-runtime
    container_name: sglang-multimodal
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: always
    privileged: false
    ports:
      - 30003:30000
    env_file: ./.env
    entrypoint: python3 -m sglang.launch_server
    command: >
      --model-path Qwen/Qwen3-VL-2B-Instruct
      --enable-multimodal
      --attention-backend flashinfer
      --dtype bfloat16
      --context-length 2048
      --mem-fraction-static 0.4
      --max-total-tokens 2048
      --max-prefill-tokens 512
      --trust-remote-code
      --log-level warning
      --host 0.0.0.0
      --port 30000
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  sglang-function:
    image: lmsysorg/sglang:latest-runtime
    container_name: sglang-function
    volumes:
      - hf_cache:/root/.cache/huggingface
    restart: always
    privileged: false
    ports:
      - 30004:30000
    entrypoint: python3 -m sglang.launch_server
    command: >
      --model-path google/functiongemma-270m-it
      --attention-backend flashinfer
      --dtype bfloat16
      --context-length 2048
      --mem-fraction-static 0.1
      --max-total-tokens 2048
      --max-prefill-tokens 512
      --trust-remote-code
      --log-level warning
      --host 0.0.0.0
      --port 30000
    ulimits:
      memlock: -1
      stack: 67108864
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

volumes:
  hf_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/huggingface
